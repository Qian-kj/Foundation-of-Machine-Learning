<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Chapter 7: AdaBoost and Ensemble Methods | Foundations of Machine Learning</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Chapter 7: AdaBoost and Ensemble Methods" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Notes for Foundations of Machine Learning" />
<meta property="og:description" content="Notes for Foundations of Machine Learning" />
<link rel="canonical" href="http://localhost:4000/notes_chapter7.html" />
<meta property="og:url" content="http://localhost:4000/notes_chapter7.html" />
<meta property="og:site_name" content="Foundations of Machine Learning" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Chapter 7: AdaBoost and Ensemble Methods" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Notes for Foundations of Machine Learning","headline":"Chapter 7: AdaBoost and Ensemble Methods","url":"http://localhost:4000/notes_chapter7.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Foundations of Machine Learning" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Foundations of Machine Learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/notes_chapter10.html">Chapter 10: Ranking</a><a class="page-link" href="/notes_chapter7.html">Chapter 7: AdaBoost and Ensemble Methods</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Chapter 7: AdaBoost and Ensemble Methods</h1>
  </header>

  <div class="post-content">
    <h1 id="chapter-7-adaboost-and-ensemble-methods">Chapter 7: AdaBoost and Ensemble Methods</h1>

<h2 id="1-aim">1. Aim</h2>
<p>Combining several predictors to create a more accurate one.</p>

<h3 id="objective1">Objective1</h3>
<p>Show how AdaBoost can reduce the empirical error of boosting and its relationship with some known algorithms.</p>

<h3 id="objective2">Objective2</h3>
<p>Generalization properties of AdaBoost based on VC-dimension and the notion of margin.</p>

<h3 id="objective3">Objective3</h3>
<p>Game-theoretic interpretation of AdaBoost for properties and equivalence analysis between weak learning assumption and a separability condition.</p>

<h2 id="2-weak-learning">2. Weak Learning</h2>
<p>For non-trivial learning tasks, it is difficult to devise an accurate algorithm satisfying the strong PAC-learning requirements. A weak learner is an algorithm that returns hypotheses slightly better than random guessing.</p>

<h2 id="3-ideas-of-boosting">3. Ideas of Boosting</h2>
<p>Transforming Weak to Strong Learners: Boosting techniques use a weak learning algorithm to create a strong learner, achieving high accuracy. Ensemble methods, by combining several weak classifiers, enhance overall predictive performance.</p>

<h3 id="definition-of-weak-learning">Definition of Weak Learning</h3>
<p>A concept class $C$ is weakly PAC-learnable if there exists an algorithm $A$, a constant $\gamma &gt; 0$, and a polynomial function $\text{poly}(\cdot, \cdot, \cdot)$ such that for any $\delta &gt; 0$, for all distributions $D$ on $X$, and for any target concept $c \in C$, the following holds for any sample size $m$:</p>

\[\Pr_{S \sim D^m}[ \Pr_{(x,y) \sim D}[ h_S(x) \neq y ] \leq 1/2 - \gamma] \geq 1 - \delta\]

<p>where $h_S$ is the hypothesis returned by algorithm $A $when trained on sample $S$,</p>

<h3 id="components-of-boosting">Components of Boosting</h3>
<ul>
  <li><strong>Base Classifiers</strong>: Weak hypotheses or models that are combined to form a more accurate predictor.</li>
  <li><strong>Combining Base Classifiers</strong>: The strategy to combine these base classifiers to improve overall accuracy is the core of boosting methods.</li>
</ul>

<h2 id="4-adaboost">4. AdaBoost</h2>

<h3 id="41-introduction">4.1 Introduction</h3>

<h3 id="process-of-adaboost-pseudocode">Process of AdaBoost (Pseudocode)</h3>

<h4 id="initialization">Initialization</h4>
<p>Start with uniform weights on the training examples.</p>

<h4 id="iteration">Iteration</h4>
<p>For each round $t$:</p>
<ul>
  <li>Train a weak learner on the weighted data.</li>
  <li>Compute the weighted error rate $\epsilon_t$,</li>
  <li>Update the weights: Increase weights for misclassified examples and decrease for correctly classified ones.</li>
</ul>

<h4 id="final-hypothesis">Final Hypothesis</h4>
<p>Combine the weak classifiers using weights proportional to their accuracy.</p>

<h3 id="key-components">Key Components</h3>
<ul>
  <li><strong>Base Classifier Set $H$</strong>: This is the set from which base classifiers are selected. Base classifiers map from the input space $X$ to the output space ${-1, +1}$,</li>
  <li><strong>Labeled Sample $S$</strong>: A sample $S$ consists of pairs $(x_i, y_i)$, where $x_i \in X$ and $y_i \in {-1, +1}$,</li>
  <li><strong>Initial Distribution $D_1$</strong>: Initially, the weight distribution $D_1$ over the training examples is uniform, meaning each example is equally likely to be chosen.</li>
</ul>

<h3 id="algorithm-steps">Algorithm Steps</h3>

<h4 id="initialize-weights">Initialize Weights</h4>
<p>Each example has an equal weight initially.</p>

<h4 id="iterate-for-t-rounds">Iterate for $T$ Rounds</h4>
<p>For each boosting round $t$:</p>
<ul>
  <li>Train Base Classifier $h_t$: Select the classifier $h_t$ that minimizes the weighted error.</li>
  <li>Compute Error $\epsilon_t$: Compute the weighted error of the classifier.</li>
  <li>Compute Classifier Weight $\alpha_t$: Compute the weight for classifier $h_t$,</li>
  <li>Update Weights: Increase weights for misclassified examples and decrease for correctly classified ones.</li>
</ul>

<h3 id="weight-update">Weight Update</h3>
<p>The weight update is performed using the formula:</p>

\[D_{t+1}(i) = \frac{D_t(i) \exp(-\alpha_t y_i h_t(x_i))}{Z_t}\]

<p>where $Z_t$ is a normalization factor ensuring $D_{t+1}$ sums to 1.</p>

<h3 id="final-classifier">Final Classifier</h3>
<p>Combine the base classifiers using their weights:</p>

\[f_T = \sum_{t=1}^{T} \alpha_t h_t\]

<h3 id="weight-distribution-d_t1i">Weight Distribution $D_{t+1}(i)$</h3>
<p>This represents the weight of the $i$-th example after $t+1$ rounds. The exponential term $e^{-y_i f_t(x_i)}$ indicates how the prediction $f_t(x_i)$ (a weighted sum of all base classifiers up to round $t$) aligns with the true label $y_i$,</p>

<h3 id="normalization-factor-z_t">Normalization Factor $Z_t$</h3>
<p>$Z_t$ ensures that the distribution $D_t$ sums to 1 over all examples. This is necessary for $D_t$ to be a valid probability distribution.</p>

<h2 id="recursive-expansion">Recursive Expansion</h2>
<p>By recursively expanding weight distribution using the definition of the distribution over the point $x_i$, we get:</p>

\[D_{t+1}(i) = \frac{D_t(i) \exp(-\alpha_t y_i h_t(x_i))}{Z_t}\]

<p>Here, $D_t(i)$ is the weight of the $i$-th example at the $t$-th round. The exponential term $e^{-\alpha_t y_i h_t(x_i)}$ adjusts the weight based on the classification performance of the base classifier $h_t$,</p>

<h2 id="generalization">Generalization</h2>

<h3 id="base-classifier-generalization">Base Classifier Generalization</h3>
<p>Allows the use of a broader range of weak learning algorithms, not limited to those that minimize weighted error.</p>

<h3 id="extended-range-of-outputs">Extended Range of Outputs</h3>
<p>Enables base classifiers to provide more detailed predictions, which can capture confidence levels or probabilities.</p>

<h3 id="flexible-coefficients">Flexible Coefficients</h3>
<p>Adapts the weight of each classifier in the final ensemble more flexibly, potentially improving the boosting process.</p>

<h3 id="non-binary-hypotheses">Non-Binary Hypotheses</h3>
<p>Uses both the sign and magnitude of the base classifier’s output, enhancing the richness of information used to form the final prediction.</p>

<h2 id="42-bound-on-the-empirical-error">4.2 Bound on the Empirical Error</h2>

<h3 id="empirical-error">Empirical Error</h3>
<p>Empirical error decreases exponentially with the number of boosting rounds.</p>

<h3 id="theorem-72">Theorem 7.2</h3>

<h4 id="theorem-breakdown">Theorem Breakdown</h4>
<p>Empirical Error: This is the error of the final classifier $f$ on the training set $S$,</p>

<h4 id="exponential-decrease">Exponential Decrease</h4>
<ul>
  <li>The empirical error decreases exponentially with the number of boosting rounds $T$,</li>
  <li>The rate of decrease depends on the terms $\epsilon_t$,</li>
</ul>

<h4 id="condition-on-gamma">Condition on $\gamma$</h4>
<ul>
  <li>If $\gamma \leq (1/2 - \epsilon_t)$ holds for all rounds $t$, the bound simplifies to $\exp(-2\gamma^2 T)$,</li>
  <li>This shows that if the weighted error $\epsilon_t$ of each classifier is consistently less than 0.5 by at least $\gamma$, the empirical error decreases rapidly.</li>
</ul>

<h4 id="proof-initial-bound-on-misclassification">Proof Initial Bound on Misclassification</h4>
<p>Use the inequality:</p>

\[\mathbb{I}[h(x) \neq y] \leq \exp(-y f(x))\]

<p>to bound the indicator function by an exponential function. This allows the empirical error to be bounded by an exponential function of the cumulative prediction:</p>

\[\sum_{i=1}^{m} D_{t+1}(i) \leq \prod_{t=1}^{T} Z_t\]

<h4 id="recursive-weight-update">Recursive Weight Update</h4>
<p>Express the bound in terms of the product of normalization factors $Z_t$:</p>

\[\prod_{t=1}^{T} Z_t \leq \exp(-2\gamma^2 T)\]

<h4 id="normalization-factor-z_t-1">Normalization Factor $Z_t$</h4>
<p>The normalization factor is expressed as:</p>

\[Z_t = \sum_{i=1}^{m} D_t(i) \exp(-\alpha_t y_i h_t(x_i))\]

<p>Simplifying this using the optimal choice of $\alpha_t$:</p>

\[Z_t = 2\sqrt{\epsilon_t(1 - \epsilon_t)}\]

<h4 id="product-of-normalization-factors">Product of Normalization Factors</h4>
<p>The product of these normalization factors over $T$ rounds gives:</p>

\[\prod_{t=1}^{T} Z_t \leq \exp(-2\gamma^2 T)\]

<h4 id="upper-bound-on-empirical-error">Upper Bound on Empirical Error</h4>
<p>Combining the results, we get:</p>

\[\sum_{i=1}^{m} \mathbb{I}[f(x_i) \neq y_i] \leq \exp(-2\gamma^2 T)\]

<h4 id="simplified-bound-with-gamma">Simplified Bound with $\gamma$</h4>
<p>If $\gamma \leq (1/2 - \epsilon_t)$, then:</p>

\[\sum_{i=1}^{m} \mathbb{I}[f(x_i) \neq y_i] \leq \exp(-2\gamma^2 T)\]

<h4 id="example">Example</h4>
<p>Suppose you have a dataset with 100 examples and you’re using AdaBoost with decision stumps (simple decision trees) as weak learners. After running the algorithm for $T$ rounds, you compute the weighted errors $\epsilon_t$ for each round. If these errors are consistently around 0.1, then:</p>

<ul>
  <li>The bound on the empirical error is:</li>
</ul>

\[\sum_{i=1}^{m} \mathbb{I}[f(x_i) \neq y_i] \leq \exp(-2\gamma^2 T)\]

<ul>
  <li>If $T = 10$, then:</li>
</ul>

\[\sum_{i=1}^{m} \mathbb{I}[f(x_i) \neq y_i] \leq \exp(-2 \cdot 0.1^2 \cdot 10) = \exp(-0.2) \approx 0.82\]

<p>This shows that with enough rounds, the empirical error can be reduced significantly.</p>

<h2 id="43-relationship-with-coordinate-descent">4.3 Relationship with Coordinate Descent</h2>

<h3 id="coordinate-descent">Coordinate Descent</h3>
<p>An optimization algorithm that minimizes a function by successively performing one-dimensional minimizations along coordinate directions. At each iteration, it selects a direction and a step size along that direction to minimize the objective function.</p>

<h3 id="objective-function-for-adaboost">Objective Function for AdaBoost</h3>
<p>Consider an ensemble function $f$ of the form:</p>

\[f(x) = \sum_{j=1}^{T} \alpha_j h_j(x)\]

<p>where $\alpha_j \geq 0$ are the coefficients and $h_j$ are the base classifiers. The objective function $F$ that AdaBoost aims to minimize is:</p>

\[F(\alpha) = \sum_{i=1}^{m} \exp(-y_i f(x_i))\]

<h3 id="convexity-and-differentiability">Convexity and Differentiability</h3>
<ul>
  <li>$F$ is a convex function of $\alpha $since it is a sum of convex functions.</li>
  <li>$F$ is differentiable because the exponential function is differentiable.</li>
</ul>

<h3 id="coordinate-descent-in-adaboost">Coordinate Descent in AdaBoost</h3>
<p>In each round $t$ of AdaBoost, a direction $e_k$ and a step size $\eta $are selected to update the coefficient vector $\alpha$,</p>

\[\alpha_{t+1} = \alpha_t + \eta e_k\]

<h3 id="ensemble-function-update">Ensemble Function Update</h3>
<p>Let $g_t$ denote the ensemble function at iteration $t$:</p>

\[g_t = g_{t-1} + \eta h_k\]

<p>The coordinate descent update $g_t = g_{t-1} + \eta h_k$ matches the AdaBoost update.</p>

<h3 id="selection-of-direction-and-step-size">Selection of Direction and Step Size</h3>
<ul>
  <li>The direction $e_k$ is selected to maximize the absolute value of the derivative of $F$:</li>
</ul>

\[e_k = \arg\max_k \left| \frac{\partial F}{\partial \alpha_k} \right|\]

<p>where,</p>

\[\frac{\partial F}{\partial \alpha_k} = -\sum_{i=1}^{m} y_i h_k(x_i) \exp(-y_i f(x_i))\]

<ul>
  <li>The step size $\eta$ is chosen to minimize $F$ along the selected direction:</li>
</ul>

\[\eta = \arg\min_{\eta} F(\alpha + \eta e_k)\]

<ul>
  <li>The step size $\eta$ is determined by setting the derivative of $F$ with respect to $\eta$ to zero:</li>
</ul>

\[\frac{\partial F}{\partial \eta} = 0\]

<h2 id="44-practical-use">4.4 Practical Use</h2>

<h3 id="choice-of-base-classifiers">Choice of Base Classifiers</h3>
<p>Decision stumps (depth-one decision trees) are frequently used with AdaBoost due to their simplicity and efficiency.</p>

<h3 id="computational-complexity">Computational Complexity</h3>
<ul>
  <li>Pre-sorting each component: $O(m \log m)$ per component, leading to $O(m N \log m)$ total.</li>
  <li>Finding the best threshold: $O(m)$ per component.</li>
  <li>Total complexity for $T$ rounds: $O(m N \log m + m N T)$,</li>
</ul>

<h3 id="performance-and-limitations">Performance and Limitations</h3>
<p>Boosting stumps can be very effective but are not universally optimal, particularly for problems that are not linearly separable (e.g., XOR problem).</p>

<h3 id="xor-problem">XOR Problem</h3>
<p>The XOR function is a binary operation that outputs true (or 1) only when the inputs differ (i.e., one is true and the other is false):</p>

<ul>
  <li>(0, 0) labeled as 0</li>
  <li>(0, 1) labeled as 1</li>
  <li>(1, 0) labeled as 1</li>
  <li>(1, 1) labeled as 0</li>
</ul>

<h2 id="5-theoretical-results">5. Theoretical Results</h2>

<h3 id="vc-dimension-based-analysis">VC-Dimension-Based Analysis</h3>
<p>Provides an upper bound on the VC-dimension of the hypothesis set used by AdaBoost, suggesting potential overfitting with large $T$,</p>

<h3 id="l1-geometric-margin">L1-Geometric Margin</h3>
<p>Defines and uses geometric margins to derive learning guarantees for AdaBoost.</p>

<h3 id="margin-based-analysis">Margin-Based Analysis</h3>
<p>Derives generalization bounds based on margins, indicating that the empirical margin loss decreases exponentially with the number of boosting rounds.</p>

<h3 id="margin-maximization">Margin Maximization</h3>
<p>Discusses the relationship between AdaBoost and margin maximization, showing that AdaBoost seeks to maximize the margin, though it may not always achieve the optimal margin.</p>

<h3 id="game-theoretic-interpretation">Game-Theoretic Interpretation</h3>
<p>Frames AdaBoost in a game-theoretic context, showing the equivalence between the weak learning assumption and the separability condition.</p>

<h2 id="51-vc-dimension-based-analysis">5.1 VC-Dimension-Based Analysis</h2>

<h3 id="recall-vc-dimension">Recall VC Dimension</h3>
<p>The VC dimension is a measure of the capacity or complexity of a set of functions or classifiers. It represents the largest number of points that can be shattered by the hypothesis class.</p>

<p>For a class of linear classifiers in 2D (lines in a plane), the VC dimension is 3. This means any three points can be shattered by lines, but not necessarily four points.</p>

<h3 id="final-hypothesis-class-f_t">Final Hypothesis Class $F_T$</h3>

<h3 id="theorem-on-vc-dimension">Theorem on VC-Dimension</h3>

<h3 id="implications">Implications</h3>
<ul>
  <li><strong>Growth of VC Dimension</strong>: The upper bound on the VC dimension grows as $O(dT \log T)$, As the number of rounds $T$ increases, the capacity of the hypothesis class $F_T$ increases, potentially leading to overfitting.</li>
  <li><strong>Generalization Error</strong>: Empirical observations show that the generalization error of AdaBoost often decreases with more rounds of boosting, so margin-based analyses are needed.</li>
</ul>

<h2 id="52-l1-geometric-margin">5.2 L1-Geometric Margin</h2>

<h3 id="geometric-margin">Geometric Margin</h3>
<p>Definition: A measure of the distance between data points and the decision boundary of a classifier.</p>

<h3 id="confidence-margin">Confidence Margin</h3>
<p>For a real-valued function $f$ at a point $x$ with label $y$, the confidence margin is defined as $y f(x)$,</p>

<h3 id="geometric-margin-for-svms">Geometric Margin for SVMs</h3>
<p>In SVMs, the geometric margin is a lower bound on the confidence margin of a linear hypothesis with a normalized weight vector $w$, where $|w|_2 = 1$,</p>

<h3 id="l1-norm">L1-Norm</h3>
<p>The Manhattan norm to measure the “size” or “length” of a vector.</p>

<h3 id="function-representation">Function Representation</h3>

<h3 id="definition-73">Definition 7.3</h3>

<h3 id="differences-from-svm">Differences from SVM</h3>
<p>Geometric margin differs from that used in the SVMs by the norm applied to the weight vector.</p>

<h3 id="implications-1">Implications</h3>

<h3 id="normalized-function">Normalized Function</h3>
<p>The normalized version of the function $f$ returned by AdaBoost is denoted as:</p>

\[\bar{f}(x) = \frac{f(x)}{\|f\|_1}\]

<h3 id="confidence-margin-and-l1-geometric-margin">Confidence Margin and L1-Geometric Margin</h3>
<p>For a correctly classified point $x$ with label $y$, the confidence margin of $\bar{f}$ at $x$ coincides with the L1-geometric margin of $f$:</p>

\[y \bar{f}(x) = \frac{y f(x)}{\|f\|_1}\]

<h3 id="convex-combination">Convex Combination</h3>
<p>Since the coefficients $\alpha_t$ are non-negative, $\rho f(x)$ is a convex combination of the base hypothesis values $h_t(x)$, If the base hypotheses $h_t$ take values in $[-1, +1]$, then $\rho f(x)$ is in $[-1, +1]$.</p>

<h2 id="53-margin-based-analysis">5.3 Margin-Based Analysis</h2>

<h3 id="key-concepts">Key Concepts</h3>

<h4 id="margins">Margins</h4>
<ul>
  <li>The margin of a classifier at a point $(x_i, y_i)$ is the product $y_i f(x_i)$, where $f(x_i)$ is the value of the classifier before taking the sign.</li>
  <li>The margin provides a measure of confidence in the prediction; larger margins indicate greater confidence.</li>
</ul>

<h4 id="rademacher-complexity">Rademacher Complexity</h4>
<ul>
  <li>Rademacher complexity is a measure of the richness of a hypothesis set $H$ of real-valued functions.</li>
  <li>For a hypothesis set $H$, the empirical Rademacher complexity $\hat{R}_S(H)$ is defined over a sample $S$,</li>
</ul>

<h4 id="convex-hull">Convex Hull</h4>
<ul>
  <li>The convex hull $\text{conv}(H)$ of a set $H$ is the smallest convex set that contains all functions in $H$,</li>
  <li>For a hypothesis set $H$ of real-valued functions, $\text{conv}(H)$ includes all convex combinations of functions in $H$,</li>
</ul>

<h3 id="rademacher-complexity-of-convex-linear-ensembles">Rademacher Complexity of Convex Linear Ensembles</h3>

<h4 id="definition">Definition</h4>
<p>For any hypothesis set $H$ of real-valued functions, its convex hull is defined by:</p>

\[\text{conv}(H) = \left\{ \sum_{i=1}^{n} \alpha_i h_i : h_i \in H, \alpha_i \geq 0, \sum_{i=1}^{n} \alpha_i = 1 \right\}\]

<h4 id="lemma-74">Lemma 7.4</h4>

<h3 id="implication">Implication</h3>

<h4 id="convh">Conv(H)</h4>
<p>Consider a hypothesis set $H = {h_1, h_2}$, where $h_1$ and $h_2$ are two different functions. The convex hull $\text{conv}(H)$ includes all functions that can be written as $\mu_1 h_1 + \mu_2 h_2$ where $\mu_1 \geq 0$, $\mu_2 \geq 0$, and $\mu_1 + \mu_2 \leq 1$, This means any function that</p>

<p>is a weighted combination of $h_1$ and $h_2$ with non-negative weights summing to at most 1 is in the convex hull.</p>

<h3 id="lemma-74-1">Lemma 7.4</h3>
<p>If the original hypothesis set $H$ has a certain capacity to fit random noise (as measured by its empirical Rademacher complexity), then forming convex combinations of functions from $H$ does not increase this capacity.</p>

<h2 id="margin-based-generalization-bounds">Margin-Based Generalization Bounds</h2>

<h3 id="theorem-77-generalization-bound">Theorem 7.7 (Generalization Bound)</h3>

<h3 id="corollary-75-ensemble-rademacher-margin-bound">Corollary 7.5 (Ensemble Rademacher Margin Bound)</h3>

<h3 id="corollary-76-ensemble-vc-dimension-margin-bound">Corollary 7.6 (Ensemble VC-Dimension Margin Bound)</h3>

<h3 id="implication-1">Implication</h3>

<h4 id="theorem-77">Theorem 7.7</h4>
<p>Provides a bound on the empirical margin loss, showing it decreases exponentially with the number of boosting rounds, as long as the weak classifiers’ errors are less than 0.5.</p>

<h4 id="corollary-75">Corollary 7.5</h4>
<p>Uses Rademacher complexity to provide a bound on the true risk, incorporating empirical margin loss and complexity measures.</p>

<h4 id="corollary-76">Corollary 7.6</h4>
<p>Uses VC-dimension to provide a bound on the true risk, incorporating empirical margin loss and capacity measures of the hypothesis set.</p>

<h2 id="generalization-error-and-margin">Generalization Error and Margin</h2>

<h3 id="empirical-margin-loss">Empirical Margin Loss</h3>
<p>Positive Edge (γ): Indicates consistent improvement over random guessing, leading to zero empirical margin loss for sufficiently large $T$,</p>

<h3 id="l1-geometric-margin-1">L1-Geometric Margin</h3>
<p>AdaBoost achieves an L1-geometric margin of $γ$, which can be as large as half the maximum geometric margin for a separable dataset.</p>

<h3 id="generalization-error">Generalization Error</h3>
<p>Decreases as the geometric margin increases, explaining the continued reduction in error even after achieving zero training error.</p>

<h3 id="maximum-margin">Maximum Margin</h3>
<p>AdaBoost does not always reach the maximum possible L1-geometric margin and may settle for a smaller margin in practice.</p>

<h2 id="54-margin-maximization">5.4 Margin Maximization</h2>

<h3 id="definition-1">Definition</h3>
<p>The maximum margin for a linearly separable sample $S = {(x_1, y_1), \ldots, (x_m, y_m)}$ is given by:</p>

\[\gamma = \max_{\alpha} \min_{i} y_i (\alpha \cdot x_i)\]

<p>where $\alpha $is the weight vector.</p>

<h3 id="optimization-problem">Optimization Problem</h3>

<h4 id="resulting-linear-program-lp">Resulting Linear Program (LP)</h4>
<p>This is a linear programming problem, a convex optimization problem with a linear objective function and linear constraints.</p>

<h4 id="solver">Solver</h4>
<p>There are various methods for solving large linear programming problems, including the Simplex method, Interior-point methods, and Special-purpose solutions.</p>

<h3 id="practical-implications">Practical Implications</h3>
<p>Despite theoretical guarantees, margin theory alone is insufficient to explain performance. In many cases, AdaBoost may outperform the LP solution despite its suboptimal margin.</p>

<h2 id="55-game-theoretic-interpretation">5.5 Game-Theoretic Interpretation</h2>

<h3 id="aim">Aim</h3>
<p>Using von Neumann’s minimax theorem to draw connections between the maximum margin, the optimal edge, and the weak learning condition.</p>

<h3 id="edge-of-a-base-classifier">Edge of a Base Classifier</h3>
<p>This measures the performance of the base classifier $h_t$ relative to random guessing under the distribution $D$,</p>

<h3 id="weak-learning-condition">Weak Learning Condition</h3>
<p>AdaBoost’s weak learning condition requires that there exists a $γ &gt; 0$ such that for any distribution $D$ over the training sample and any base classifier $h_t$, the edge $γ_t(D)$ is at least $γ$:</p>

\[\gamma_t(D) = \mathbb{E}_{(x,y) \sim D}[y h_t(x)] \geq \gamma\]

<h3 id="definition-79">Definition 7.9</h3>

<h3 id="zero-sum-game">Zero-Sum Game</h3>
<p>In Boosting, the row player selects training instances, and the column player selects base classifiers.</p>

<h3 id="definition-710">Definition 7.10</h3>

<h3 id="mixed-strategy">Mixed Strategy</h3>
<p>A mixed strategy for the row player is a distribution $D$ over the training points. A mixed strategy for the column player is a distribution over the base classifiers, derived from a non-negative vector $\alpha$,</p>

<h3 id="theorem-711-von-neumanns-minimax-theorem">Theorem 7.11 von Neumann’s Minimax Theorem</h3>
<p>This equality shows that there exists a mixed strategy for each player such that the expected loss for one is the same as the expected payoff for the other.</p>

<h3 id="adaboost-as-a-zero-sum-game">AdaBoost as a Zero-Sum Game</h3>

<h3 id="formula">Formula</h3>

<h3 id="the-equivalence">The Equivalence</h3>
<p>Relates the maximum margin and the best possible edge.</p>

<h3 id="56-implications">5.6 Implications</h3>

<h4 id="weak-learning-condition-and-margin">Weak Learning Condition and Margin</h4>
<p>The weak learning condition ($γ^* &gt; 0$) implies $ρ^* &gt; 0$, indicating the existence of a classifier with a positive margin.</p>

<h4 id="algorithmic-goals">Algorithmic Goals</h4>
<p>AdaBoost aims to achieve a non-zero margin, although it may not always achieve the optimal margin.</p>

<h4 id="strength-of-weak-learning-assumption">Strength of Weak Learning Assumption</h4>
<p>While appearing weak, the weak learning assumption is strong as it implies linear separability with a margin $2γ^* &gt; 0$, a condition not often met in practical datasets.</p>

<h2 id="l1-regularization">L1-Regularization</h2>

<h3 id="motivation-issues">Motivation Issues</h3>
<ul>
  <li>When the training sample is not linearly separable, AdaBoost may not achieve a positive edge, meaning the weak learning condition does not hold.</li>
  <li>Even if AdaBoost does achieve a positive edge, it may be very small ($γ$ is very small). This can cause the algorithm to focus excessively on hard-to-classify examples, leading to large mixture weights for some base classifiers.</li>
</ul>

<h3 id="consequences">Consequences</h3>
<ul>
  <li>This concentration on a few hard examples can result in a poor overall performance, as a few base classifiers with large weights dominate the final ensemble.</li>
  <li>The resulting classifier may heavily rely on these few classifiers, leading to overfitting and reduced generalization performance.</li>
</ul>

<h3 id="solutions-l1-regularized-adaboost">Solutions: L1-Regularized AdaBoost</h3>

<h4 id="early-stopping">Early-Stopping</h4>
<p>One method to prevent these issues is early-stopping, which involves limiting the number of boosting rounds $T$,</p>

<h4 id="l1-regularization-1">L1-Regularization</h4>
<p>Another effective method is to control the magnitude of the mixture weights by incorporating a regularization term into the objective function of AdaBoost. The regularization term is based on the L1-norm of the weight vector, leading to what is known as L1-regularized AdaBoost.</p>

<h3 id="objective-function-explanation">Objective Function Explanation</h3>
<p>The term $\sum_{i=1}^{m} \exp(-y_i f(x_i))$ is the same as the original AdaBoost objective function, representing the empirical risk. The term $λ | \alpha |_1$ is the regularization term, with $λ$ being a regularization parameter that controls the trade-off between fitting the training data and keeping the weights small.</p>

<h3 id="optimization">Optimization</h3>
<p>The objective function $G $is convex, allowing for efficient optimization methods such as coordinate descent to find the optimal weights.</p>

<h2 id="generalization-guarantees">Generalization Guarantees</h2>

<h3 id="inequality-1">Inequality 1</h3>
<p>True risk $\leq$ Empirical margin loss + Rademacher Complexity + Confidence Terms</p>

<h3 id="inequality-2">Inequality 2</h3>
<p>For $ρ &gt; 1$ The bound trivially holds because the first term on the right-hand side is zero.</p>

<h3 id="hölders-inequality">Hölder’s Inequality</h3>
<p>Let $p$ and $q$ be conjugate exponents, meaning that they satisfy:</p>

\[\frac{1}{p} + \frac{1}{q} = 1\]

<p>For any sequences ${a_i}$ and ${b_i}$, Hölder’s Inequality states that:</p>

\[\sum_{i} |a_i b_i| \leq \left( \sum_{i} |a_i|^p \right)^{1/p} \left( \sum_{i} |b_i|^q \right)^{1/q}\]

<h3 id="using-hölders-inequality">Using Hölder’s Inequality</h3>

<h3 id="inequality-3">Inequality 3</h3>
<p>This form of the bound uses the general upper bound $\sum_{i} \exp(a_i) \leq m \exp \left( \frac{1}{m} \sum_{i} a_i \right)$ for all $u \in \mathbb{R}$,</p>

<h3 id="inequality-4">Inequality 4</h3>
<p>This bound is useful for deriving an optimal strategy for selecting the weights $\alpha$ and the parameter $ρ$,</p>

<h2 id="optimization-1">Optimization</h2>

<h3 id="selecting-weights-alpha">Selecting Weights $\alpha$</h3>
<p>The minimization with respect to $ρ$ does not lead to a convex optimization problem due to the interdependence of the second and third terms. Instead, $ρ$ is treated as a free parameter, typically determined via cross-validation.</p>

<h3 id="optimization-for-alpha">Optimization for $\alpha$</h3>
<p>The primary focus is on minimizing the empirical risk. The bound suggests selecting $\alpha $as the solution to the following problem:</p>

\[\min_{\alpha} \sum_{i=1}^{m} \exp(-y_i f(x_i)) + λ \| \alpha \|_1\]

<h3 id="lagrange-formulation">Lagrange Formulation</h3>
<p>We want to find the weights $\alpha$ that minimize the empirical risk of the classifier while also satisfying the constraint on the L1-norm of $\alpha$, So,</p>

\[\min_{\alpha} \sum_{i=1}^{m} \exp(-y_i f(x_i)) \quad \text{s.t.} \quad \| \alpha \|_1 \leq C\]

<h2 id="implication-2">Implication</h2>
<p>Controlling the L1-norm of the weight vector can achieve a better generalization error bound compared to the standard AdaBoost when the data is not linearly separable or the edge is very small.</p>

<h2 id="6-takeaways">6. Takeaways</h2>
<ul>
  <li><strong>Simplicity</strong>: AdaBoost is simple to implement and computationally efficient.</li>
  <li><strong>Theoretical Foundations</strong>: Supported by robust theoretical analysis, although it doesn’t always achieve maximum margin, indicating potential for refined theoretical insights.</li>
  <li><strong>Parameter Selection</strong>: Critical parameters include the number of boosting rounds (T) and the choice of base classifiers.</li>
  <li><strong>Noise Sensitivity</strong>: Sensitive to noise, as it increases the weight of misclassified examples. Regularization techniques like L1-regularization and alternative loss functions (e.g., logistic loss) can mitigate this.</li>
  <li><strong>Outlier Detection</strong>: AdaBoost’s propensity to assign large weights to hard-to-classify examples can be used for detecting outliers.</li>
  <li><strong>Base Classifiers</strong>: Decision stumps are a common choice due to their simplicity and speed.</li>
  <li><strong>Computational Complexity</strong>: Efficient in practice.</li>
  <li><strong>Handling Non-Separable Data</strong>: Regularization methods and less aggressive loss functions help in managing non-separable data.</li>
</ul>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Foundations of Machine Learning</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Foundations of Machine Learning</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Notes for Foundations of Machine Learning</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
